---
layout:     post
title:      大数据集群安装
subtitle:   hadoop 三节点集群安装
date:       2018-07-27
author:     zhangzhongxin
header-img: img/post-bg-universe.jpg
catalog: true
tags:
    - 大数据
---


## 前言

这篇博客 早就想写了 架构师也一直叫我补文档，一直没时间，现在终于补上了

一般公司集群都是由运维部门统一管理，自己部门的小规模集群只用于测试，
这里 hadoop 集群 我只搭建了非高可用版本，想要高可用版本的同学，可以私聊我

## 正文 
一.hadoop 版本：
```objc
    最开始接触hadoop 的时候，都是用的apache源生版本，但是源生版本有很多版本冲突问题，cdh 版本帮我们解决掉了这些冲突，
而且还增加了很多工具类，运行起来也很稳定，我这里hadoop 集群搭建的版本是：hadoop-2.6.0-cdh5.7.0 (非常稳定的版本)

```
二.基础环境：
```objc
1.更改主机hostname eg： hadoop01 hadoop02 hadoop03
2.配置服务器ip为固定不变的ip,(/etc/sysconfig/network-scripts/ifcfg-eth0),如果是公司网络，要申请运维部门来绑定
3.在 /etc/hosts 中 添加 ip hostname 的映射
4.关闭防火墙
service iptables stop
chkconfig iptables off
vi /etc/selinux/config
SELINUX=disabled
```

三.配置ssh 
```objc
1、首先在三台机器上配置对本机的ssh免密码登录
生成本机的公钥，过程中不断敲回车即可，ssh-keygen命令默认会将公钥放在/root/.ssh目录下
ssh-keygen -t rsa
将公钥复制为authorized_keys文件，此时使用ssh连接本机就不需要输入密码了
cd /root/.ssh
cp id_rsa.pub authorized_keys
2、接着配置三台机器互相之间的ssh免密码登录
使用ssh-copy-id -i spark命令将本机的公钥拷贝到指定机器的authorized_keys文件中（方便好用）
```
四.安装hadoop 集群
```objc
1、去 http://archive.cloudera.com/cdh5/cdh/5/ 地址下载 hadoop-2.6.0-cdh5.7.0.tar.gz 
2、将hadoop包进行解压缩：tar -zxvf hadoop-2.6.0-cdh5.7.0.tar.gz(统一安装在一个目录下 eg:/data/workdir)
3、对hadoop目录进行重命名：mv hadoop-2.4.1 hadoop
4、配置hadoop相关环境变量
vi ~/.bash_profile
export HADOOP_HOME=/data/workdir/hadoop
export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin
source ~/.bash_profile

(1)修改 core-site.xml
<!-- hdfs 端口 -->
<property>
  <name>fs.default.name</name>
  <value>hdfs://hadoop01:9000</value>
</property>
<!-- 这个最好修改，默认放 /tmp 下，机器重启，自动消失 -->
<property>
  <name>hadoop.tmp.dir</name>
  <value>/data/app/data/tmp</value>
</property>
<!--指定可以在任何 IP 访问-->
    <property>
        <name>hadoop.proxyuser.hduser.hosts</name>
        <value>*</value>
    </property>
    <!--指定所有用户可以访问-->
    <property>
        <name>hadoop.proxyuser.hduser.groups</name>
        <value>*</value>
    </property>
(2) 修改hdfs-site
<!-- namenode 存储路径 -->
<property>
  <name>dfs.name.dir</name>
  <value>/data/workdir/data/namenode</value>
</property>
<!-- datanode 存储路径 -->
<property>
  <name>dfs.data.dir</name>
  <value>/data/workdir/data/datanode</value>
</property>
<!-- hdfs 临时文件 存储路径 -->
<property>
  <name>dfs.tmp.dir</name>
  <value>/data/workdir/data/tmp</value>
</property>
<!-- hdfs 副本数 -->
<property>
  <name>dfs.replication</name>
  <value>3</value>
</property>

（3）mapred-site.xml
<!-- 指定调度方式 -->
<property>
  <name>mapreduce.framework.name</name>
  <value>yarn</value>
</property>

(4) 修改yarn-site.xml
<!-- yarn 主节点 -->
<property>
  <name>yarn.resourcemanager.hostname</name>
  <value>hadoop01</value>
</property>
<!-- yarn 计算方式 -->
<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>mapreduce_shuffle</value>
</property>

(5) 修改slave 文件
hadoop01
hadoop02
hadoop03

5.拷贝 hadoop 包
1、使用如上配置在另外两台机器上搭建hadoop，可以使用scp命令将hadoop01 上面的hadoop安装包和 ~/.bash_profile 配置文件都拷贝过去。
2、要记得对~/.bash_profile 文件进行source，以让它生效。
3、记得在hadoop02和hadoop03的/data/workdir 目录下创建data目录。

6.启动hdfs 
(1) 格式化namenode：在spark1上执行以下命令，hadoop namenode -format
(2) 启动hdfs集群：start-dfs.sh
(3) 验证启动是否成功：jps、50070端口
(4) hadoop-env.sh 中JAVA_HOME 改为绝对路径

1、启动yarn集群：start-yarn.sh
2、验证启动是否成功：jps、8088端口
hadoop01：resourcemanager、nodemanager
hadoop02：nodemanager
hadoop03：nodemanager

```


